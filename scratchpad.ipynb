{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "furnished-peace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "killing-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "concrete-married",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = data = d2 = \\\n",
    "    tf.keras.datasets.boston_housing.load_data( test_split=0.2)\n",
    "d2[0][0].shape, d2[0][1].shape,d2[1][0].shape,d2[1][1].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "existing-printer",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in d:\n",
    " for j in i:\n",
    "    print (j.shape)\n",
    "'''\n",
    "Shape: \n",
    "(60000, 28, 28)\n",
    "(60000,)\n",
    "(10000, 28, 28)\n",
    "(10000,)\n",
    "\n",
    " training: 60K 28x28 images, 60K labels\n",
    " testing: 10K 28x28 images, 10K labels\n",
    "'''\n",
    "d[0][1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "latin-dutch",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module tensorflow.python.keras.layers.core:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(tf.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Convolution2D)\n",
    "help(tf.keras.layers.Dense)\n",
    "help(tf.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "twelve-victor",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method get_weights in module tensorflow.python.keras.engine.base_layer:\n",
      "\n",
      "get_weights() method of tensorflow.python.keras.layers.core.Dense instance\n",
      "    Returns the current weights of the layer.\n",
      "    \n",
      "    The weights of a layer represent the state of the layer. This function\n",
      "    returns both trainable and non-trainable weight values associated with this\n",
      "    layer as a list of Numpy arrays, which can in turn be used to load state\n",
      "    into similarly parameterized layers.\n",
      "    \n",
      "    For example, a Dense layer returns a list of two values-- per-output\n",
      "    weights and the bias value. These can be used to set the weights of another\n",
      "    Dense layer:\n",
      "    \n",
      "    >>> a = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(1.))\n",
      "    >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      "    >>> a.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b = tf.keras.layers.Dense(1,\n",
      "    ...   kernel_initializer=tf.constant_initializer(2.))\n",
      "    >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      "    >>> b.get_weights()\n",
      "    [array([[2.],\n",
      "           [2.],\n",
      "           [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    >>> b.set_weights(a.get_weights())\n",
      "    >>> b.get_weights()\n",
      "    [array([[1.],\n",
      "           [1.],\n",
      "           [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      "    \n",
      "    Returns:\n",
      "        Weights values as a list of numpy arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(1,2,3))\n",
    "layer1 = tf.keras.layers.Dense( 16, activation='relu')\n",
    "layer1.trainable=False\n",
    "help(layer1.get_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "animated-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(8, 3)]                  0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (8, 32)                   128       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (8, 2)                    66        \n",
      "=================================================================\n",
      "Total params: 194\n",
      "Trainable params: 194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(3,), batch_size=8)\n",
    "layer1 = tf.keras.layers.Dense( 32, activation='relu')(inputs)\n",
    "outputs = tf.keras.layers.Dense(2, activation='tanh')(layer1)\n",
    "sanity_model = tf.keras.Model(inputs, outputs)\n",
    "sanity_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "purple-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 32)           448         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 32)           448         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 64)           0           dense_35[0][0]                   \n",
      "                                                                 dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            65          concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 961\n",
      "Trainable params: 513\n",
      "Non-trainable params: 448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(13,))\n",
    "layer1a = tf.keras.layers.Dense( 32, activation='relu')(inputs)\n",
    "layer1b = tf.keras.layers.Dense( 32, activation='relu')\n",
    "layer1c = layer1b(inputs)\n",
    "layer1b.trainable=False\n",
    "layer1 = tf.keras.layers.Concatenate()([layer1a,layer1c])\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(layer1)\n",
    "concat_model = tf.keras.Model(inputs, outputs)\n",
    "concat_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "floral-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer='adam',\n",
    "        loss=loss_fn,\n",
    "        metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "liberal-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [8,2] vs. [32,2]\n\t [[node gradient_tape/mean_squared_error/mul_1 (defined at <ipython-input-113-df940825c06a>:2) ]] [Op:__inference_train_function_3060]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-14ceccec8066>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [8,2] vs. [32,2]\n\t [[node gradient_tape/mean_squared_error/mul_1 (defined at <ipython-input-113-df940825c06a>:2) ]] [Op:__inference_train_function_3060]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    " tf. convert_to_tensor(y_test)\n",
    "history=model.fit(x_train, y_train, epochs=5)\n",
    "help(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "original-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = np.array([[ 0.00416565 , -0.00837508,  -0.0049096 ,  -0.00106102,  -0.01659322,  -0.01465032],\n",
    " [ 0.         ,  0.        ,   0.        ,   0.        ,   0.        ,   0.        ],\n",
    " [ 0.         ,  0.        ,   0.        ,   0.        ,   0.        ,   0.        ],\n",
    " [ 0.07261965 , -0.10078779,  -0.07817969,  -0.01313207,  -0.1799222 ,  -0.17855012],\n",
    " [ 0.07261968 , -0.10078776,  -0.07817972,  -0.01313227,  -0.17992216,  -0.1785503 ],\n",
    " [ 0.07261968 , -0.10078782,  -0.07817972,  -0.01313192,  -0.17992216,  -0.17855027],\n",
    " [ 0.03783834 , -0.08479811,  -0.07477683,  -0.01795623,  -0.14150071,  -0.14370002],\n",
    " [ 0.07261965 , -0.10078776,  -0.07817966,  -0.01313192,  -0.17992212,  -0.17855027],\n",
    " [ 0.04993503 , -0.0788278 ,  -0.07251662,  -0.01501057,  -0.13326663,  -0.14417839],\n",
    " [ 0.06670453 , -0.09810309,  -0.07757652,  -0.01393871,  -0.17242712,  -0.17226571],\n",
    " [ 0.05155933 , -0.0681935 ,  -0.05890559,  -0.01091504,  -0.11933175,  -0.12890205]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dental-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "detected-shore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_hist = {}\n",
    "X = list(np.ndarray.flatten( data_5 ))\n",
    "for xi in X:\n",
    "    if xi in d_hist: d_hist[xi]+=1\n",
    "    else: d_hist[xi]=1\n",
    "\n",
    "d_arr = [ d_hist[k] for k in sorted(d_hist.keys())]\n",
    "plt.scatter(d_hist.keys(), d_arr)\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 20) # (w, h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dental-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.functional.Functional instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            Note that the progress bar is not particularly useful when\n",
      "            logged to a file, so verbose=2 is recommended when not running\n",
      "            interactively (eg, in a production environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
      "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
      "            and need not be passed into `model.fit`.\n",
      "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "            `verbose` argument to `model.fit`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using `validation_split`\n",
      "            or `validation_data` is not affected by regularization layers like\n",
      "            noise and dropout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "              - dataset\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` could be provided.\n",
      "            Note that `validation_data` does not support all the data types that\n",
      "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is ignored\n",
      "            when `x` is a generator. 'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample. This\n",
      "            argument is not supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            When passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument. This argument is not supported with\n",
      "            array inputs.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of an infinitely repeated dataset, it will run into an\n",
      "            infinite loop. If 'validation_steps' is specified and only part of\n",
      "            the dataset will be consumed, the evaluation will start from the\n",
      "            beginning of the dataset at each epoch. This ensures that the same\n",
      "            validation samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: 1. If the model was never compiled or,\n",
      "        2. If `model.fit` is  wrapped in `tf.function`.\n",
      "    \n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects or when the input data is empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.ones(shape=(3,))\n",
    "#np.flatten(inputs)\n",
    "help(model.fit)\n",
    "tf.keras.losses.Huber"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
